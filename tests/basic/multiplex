#!/bin/bash
#This test tests that self-heals don't perform fsync when durability is turned
#off

. $(dirname $0)/../include.rc
. $(dirname $0)/../volume.rc

function count_processes {
	# -x means exact, so we don't get glusterd, glusterfs, etc.
	pgrep -x glusterfsd | wc -w
}

function get_brick_pids {
	$CLI volume status $V0 | grep "^Brick " | awk '{print $6}' \
			       | grep -v "N/A"
}

function count_up_bricks {
        get_brick_pids | wc -l
}

function count_brick_pids {
        get_brick_pids | sort | uniq | wc -l
}

TEST glusterd
TEST $CLI volume create $V0 $H0:$B0/brick{0,1}

TEST $CLI volume start $V0
# Without multiplexing, there would be two.
EXPECT 1 count_processes
EXPECT 2 count_up_bricks

TEST $CLI volume stop $V0
EXPECT_WITHIN $PROCESS_DOWN_TIMEOUT 0 count_processes
TEST $CLI volume start $V0
EXPECT 1 count_processes
EXPECT 2 count_up_bricks

TEST kill_brick $V0 $H0 $B0/brick1
# Make sure the whole process didn't go away.
EXPECT 1 count_processes
EXPECT 1 count_up_bricks

TEST $CLI volume start $V0 force
EXPECT 1 count_processes
EXPECT 2 count_up_bricks

# Killing the first brick is a bit more of a challenge due to socket-path
# issues.
TEST kill_brick $V0 $H0 $B0/brick0
EXPECT 1 count_processes
EXPECT 1 count_up_bricks
TEST $CLI volume start $V0 force
EXPECT 1 count_processes
EXPECT 2 count_up_bricks

# Make sure that the two bricks show the same PID.
EXPECT 1 count_brick_pids

# Do a quick test to make sure that the bricks are acting as separate bricks
# even though they're in the same process.
TEST $GFS --volfile-id=$V0 --volfile-server=$H0 $M0
for i in $(seq 10 99); do
        echo hello > $M0/file$i
done
nbrick0=$(ls $B0/brick0/file?? | wc -l)
nbrick1=$(ls $B0/brick1/file?? | wc -l)
TEST [ $((nbrick0 + nbrick1)) -eq 90 ]
TEST [ $((nbrick0 * nbrick1)) -ne 0 ]

cleanup
